# Curriculum Learning for Natural Language Understanding
This work reproduces the results presented in the [Curriculum Learning for Natural Language Understanding paper](https://aclanthology.org/2020.acl-main.542/). It aims to demonstrate the effect of curriculum learning on the performance of the BERT language model in machine reading comprehension using the SQuAD 2.0 dataset. The implementation of the **difficulty evaluation** step of the curriculum learning framework trains 10 (N = 10) models on 10 distinct portions of the training set. Then, **curriculum arrangement** sorts the 10 splits of the training set by difficulty (F1 as the golden metric), trains BERT through the 10 difficulty stages for 1 epoch each, and concludes by training it on the original distribution in the train set until it converges.
