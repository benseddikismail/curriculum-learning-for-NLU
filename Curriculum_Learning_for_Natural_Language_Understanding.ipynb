{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g50JWXbB2ze"
      },
      "source": [
        "# Curriculum Learning for Natural Language Understanding\n",
        "\n",
        "The following is an attempt at reproducing the results presented in the [Curriculum Learning for Natural Language Understanding paper](https://aclanthology.org/2020.acl-main.542/). It aims to demonstrate the effect of curriculum learning on the performance of the BERT language model in machine reading comprehension using the SQuAD 2.0 dataset. The implementation of the **difficulty evaluation** step of the curriculum learning framework trains 10 (N = 10) models on 10 distinct portions of the training set. Then, **curriculum arrangement** sorts the 10 splits of the training set by difficulty (F1 as the golden metric), trains BERT through the 10 difficulty stages for 1 epoch each, and concludes by training it on the original distribution in the train set until it converges."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminaries"
      ],
      "metadata": {
        "id": "QjtBjb9FFlOq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7j9F1YTMFuE",
        "outputId": "82ce4f9a-8759-4d4f-e5e8-4aa30570443d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from numpy import unravel_index\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import random\n",
        "import sys\n",
        "from IPython.display import Image\n",
        "import time\n",
        "from transformers import BertTokenizerFast, BertTokenizer, BertForQuestionAnswering\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "!CUBLAS_WORKSPACE_CONFIG=:4096:2\n",
        "\n",
        "!pip install transformers\n",
        "\n",
        "# reproducibility\n",
        "def set_seed(seed = 1234):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.enabled = False\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # torch.use_deterministic_algorithms(False)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKgVAn96ntQj",
        "outputId": "499a89cd-0fa3-4ee1-d607-0b42f4adf855"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63U7HoojY7d4"
      },
      "source": [
        "## SQuAD 2.0 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ITSp_qMhon65"
      },
      "outputs": [],
      "source": [
        "squad_dataset = load_dataset(\"squad_v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "mVUMMoVoyTRy"
      },
      "outputs": [],
      "source": [
        "train_squad = squad_dataset[\"train\"].shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "sXoYvIi3yTRy"
      },
      "outputs": [],
      "source": [
        "validation_squad = squad_dataset[\"validation\"].shuffle(seed=42).select(range(1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9G1jIkwxj5EQ",
        "outputId": "f1a40225-4adc-4cd3-a211-5c36098cb05d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ],
      "source": [
        "train_squad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QWlfsFA9lvc",
        "outputId": "6b28235e-a1b3-43ac-a785-e0b1898106e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56e0f3907aa994140058e80a',\n",
              " 'title': 'Canon_law',\n",
              " 'context': 'The Roman Catholic Church canon law also includes the main five rites (groups) of churches which are in full union with the Roman Catholic Church and the Supreme Pontiff:',\n",
              " 'question': 'What term characterizes the intersection of the rites with the Roman Catholic Church?',\n",
              " 'answers': {'text': ['full union'], 'answer_start': [104]}}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "train_squad[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVVrUwynQGbg",
        "outputId": "8f04d155-cc34-4492-8c7c-b168791a7deb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "validation_squad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LvIVJ9KwYkP",
        "outputId": "9c41dc32-c63b-498d-833b-8de14411ffe3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5733ea04d058e614000b6595',\n",
              " 'title': 'French_and_Indian_War',\n",
              " 'context': \"In the spring of 1753, Paul Marin de la Malgue was given command of a 2,000-man force of Troupes de la Marine and Indians. His orders were to protect the King's land in the Ohio Valley from the British. Marin followed the route that Céloron had mapped out four years earlier, but where Céloron had limited the record of French claims to the burial of lead plates, Marin constructed and garrisoned forts. He first constructed Fort Presque Isle (near present-day Erie, Pennsylvania) on Lake Erie's south shore. He had a road built to the headwaters of LeBoeuf Creek. Marin constructed a second fort at Fort Le Boeuf (present-day Waterford, Pennsylvania), designed to guard the headwaters of LeBoeuf Creek. As he moved south, he drove off or captured British traders, alarming both the British and the Iroquois. Tanaghrisson, a chief of the Mingo, who were remnants of Iroquois and other tribes who had been driven west by colonial expansion. He intensely disliked the French (whom he accused of killing and eating his father). Traveling to Fort Le Boeuf, he threatened the French with military action, which Marin contemptuously dismissed.\",\n",
              " 'question': 'Where did Marin build first fort?',\n",
              " 'answers': {'text': ['Fort Presque Isle (near present-day Erie, Pennsylvania',\n",
              "   'Fort Presque Isle',\n",
              "   'near present-day Erie, Pennsylvania',\n",
              "   'Fort Presque Isle',\n",
              "   'near present-day Erie, Pennsylvania'],\n",
              "  'answer_start': [425, 425, 444, 425, 444]}}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "validation_squad[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DauZRzlKhglZ"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "eb-rAE2JHYO5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huXJOvg9wYkR",
        "outputId": "50dc46b8-c0f1-4c5f-ba83-fb2cad5adf09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '571adcf932177014007e9f56',\n",
              " 'title': 'Athanasius_of_Alexandria',\n",
              " 'context': \"Alexandria was the most important trade center in the whole empire during Athanasius's boyhood. Intellectually, morally, and politically—it epitomized the ethnically diverse Graeco-Roman world, even more than Rome or Constantinople, Antioch or Marseilles. Its famous catechetical school, while sacrificing none of its famous passion for orthodoxy since the days of Pantaenus, Clement of Alexandria, Origen of Alexandria, Dionysius and Theognostus, had begun to take on an almost secular character in the comprehensiveness of its interests, and had counted influential pagans among its serious auditors.\",\n",
              " 'question': 'What was Alexandria known for?',\n",
              " 'answers': {'text': ['important trade center'], 'answer_start': [24]}}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "train_squad[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "EltHOea8wYkR"
      },
      "outputs": [],
      "source": [
        "def find_end(example):\n",
        "\n",
        "    if (len(example['answers']['text']) != 0):\n",
        "        context = example['context']\n",
        "        text = example['answers']['text'][0]\n",
        "        start_idx = example['answers']['answer_start'][0]\n",
        "\n",
        "        end_idx = start_idx + len(text)\n",
        "\n",
        "        temp = example['answers'] # to change the value\n",
        "        temp['answer_end']=end_idx\n",
        "        temp['answer_start'] = start_idx # [num]->num\n",
        "        temp['text'] = text # ['text']->text\n",
        "\n",
        "    else:\n",
        "        temp = example['answers']\n",
        "        temp['answer_end'] = 0 # []->0\n",
        "        temp['answer_start'] = 0 # []->0\n",
        "        temp['text'] = \"\" # []->\"\"\n",
        "\n",
        "    return example\n",
        "\n",
        "train_squad = train_squad.map(find_end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2Y_bTeSwYkR",
        "outputId": "0a012e83-6347-4041-b73b-3ed460d3fec7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '571adcf932177014007e9f56',\n",
              " 'title': 'Athanasius_of_Alexandria',\n",
              " 'context': \"Alexandria was the most important trade center in the whole empire during Athanasius's boyhood. Intellectually, morally, and politically—it epitomized the ethnically diverse Graeco-Roman world, even more than Rome or Constantinople, Antioch or Marseilles. Its famous catechetical school, while sacrificing none of its famous passion for orthodoxy since the days of Pantaenus, Clement of Alexandria, Origen of Alexandria, Dionysius and Theognostus, had begun to take on an almost secular character in the comprehensiveness of its interests, and had counted influential pagans among its serious auditors.\",\n",
              " 'question': 'What was Alexandria known for?',\n",
              " 'answers': {'answer_end': 46,\n",
              "  'answer_start': 24,\n",
              "  'text': 'important trade center'}}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "train_squad[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syVkhDaQwYkS",
        "outputId": "8954b925-17b4-4530-83e4-f57caaf2cc94"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56fb84ebb28b3419009f1de7',\n",
              " 'title': 'Middle_Ages',\n",
              " 'context': 'During this period the practice of manuscript illumination gradually passed from monasteries to lay workshops, so that according to Janetta Benton \"by 1300 most monks bought their books in shops\", and the book of hours developed as a form of devotional book for lay-people. Metalwork continued to be the most prestigious form of art, with Limoges enamel a popular and relatively affordable option for objects such as reliquaries and crosses. In Italy the innovations of Cimabue and Duccio, followed by the Trecento master Giotto (d. 1337), greatly increased the sophistication and status of panel painting and fresco. Increasing prosperity during the 12th century resulted in greater production of secular art; many carved ivory objects such as gaming-pieces, combs, and small religious figures have survived.',\n",
              " 'question': 'What were many pieces of secular art carved from in this period?',\n",
              " 'answers': {'answer_end': 728, 'answer_start': 723, 'text': 'ivory'}}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "train_squad[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRzCbKA-wYkS",
        "outputId": "d687e0f6-44c2-4c89-bf60-111e47ff178d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '5aceab0f32bba1001ae4af93',\n",
              " 'title': 'Jews',\n",
              " 'context': 'For centuries, Jews worldwide have spoken the local or dominant languages of the regions they migrated to, often developing distinctive dialectal forms or branches that became independent languages. Yiddish is the Judæo-German language developed by Ashkenazi Jews who migrated to Central Europe. Ladino is the Judæo-Spanish language developed by Sephardic Jews who migrated to the Iberian peninsula. Due to many factors, including the impact of the Holocaust on European Jewry, the Jewish exodus from Arab and Muslim countries, and widespread emigration from other Jewish communities around the world, ancient and distinct Jewish languages of several communities, including Judæo-Georgian, Judæo-Arabic, Judæo-Berber, Krymchak, Judæo-Malayalam and many others, have largely fallen out of use.',\n",
              " 'question': 'For how long have Jews not spoken the local languages of the regions they migrated to?',\n",
              " 'answers': {'answer_end': 0, 'answer_start': 0, 'text': ''}}"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "train_squad[-10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "M7hNs_uSwYkS"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "Hi-wpHi8wYkS"
      },
      "outputs": [],
      "source": [
        "def process_train(train_squad):\n",
        "\n",
        "  tokenized_train = tokenizer(train_squad['context'], train_squad['question'], truncation=True, padding=True)\n",
        "\n",
        "  def find_token_indexes(tokenized, dataset):\n",
        "      start_token_list = []\n",
        "      end_token_list = []\n",
        "      answers = dataset['answers']\n",
        "      for i in range(len(answers)):\n",
        "          if (answers[i]['text'] != ''):\n",
        "              start_token = tokenized.char_to_token(i, answers[i]['answer_start'])\n",
        "              end_token = tokenized.char_to_token(i, answers[i]['answer_end'] - 1)\n",
        "\n",
        "              if start_token is None:\n",
        "                  start_token = tokenizer.model_max_length\n",
        "              if end_token is None:\n",
        "                  end_token = tokenizer.model_max_length\n",
        "          else:\n",
        "              start_token = 0\n",
        "              end_token = 0\n",
        "\n",
        "          start_token_list.append(start_token)\n",
        "          end_token_list.append(end_token)\n",
        "\n",
        "      return start_token_list, end_token_list\n",
        "\n",
        "  s, e = find_token_indexes(tokenized_train, train_squad)\n",
        "  train_squad = train_squad.add_column(\"start_position\", s)\n",
        "  train_squad = train_squad.add_column(\"end_position\", e)\n",
        "\n",
        "  return (tokenized_train, train_squad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train, processed_train_squad = process_train(train_squad)"
      ],
      "metadata": {
        "id": "zyKAW-OYL9zK"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWE2ASaewYkS",
        "outputId": "91f74096-85e2-47b1-b485-620f764b7243"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers', 'start_position', 'end_position'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "processed_train_squad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8"
      ],
      "metadata": {
        "id": "aEm4f48-4eZT"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "0e4Pv5Kshgla"
      },
      "outputs": [],
      "source": [
        "def get_train_dataloader(tokenized_train, train_squad, batch_size):\n",
        "  train_data = TensorDataset(torch.tensor(tokenized_train['input_ids'], dtype=torch.int64),\n",
        "                            torch.tensor(tokenized_train['token_type_ids'], dtype=torch.int64),\n",
        "                            torch.tensor(tokenized_train['attention_mask'], dtype=torch.float),\n",
        "                            torch.tensor(train_squad['start_position'], dtype=torch.int64),\n",
        "                            torch.tensor(train_squad['start_position'], dtype=torch.int64))\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  return train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = get_train_dataloader(tokenized_train, processed_train_squad, batch_size)"
      ],
      "metadata": {
        "id": "PXzheKXPND9T"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation"
      ],
      "metadata": {
        "id": "NgZGDvjCHU_D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "kxMPkF2SwYkT"
      },
      "outputs": [],
      "source": [
        "def get_val_dataloader(validation_squad, batch_size):\n",
        "  tokenized_validation = tokenizer(validation_squad['context'], validation_squad['question'], truncation=True, padding=True, return_offsets_mapping=True)\n",
        "  val_data = TensorDataset(torch.tensor(tokenized_validation['input_ids'], dtype=torch.int64),\n",
        "                          torch.tensor(tokenized_validation['token_type_ids'], dtype=torch.int64),\n",
        "                          torch.tensor(tokenized_validation['attention_mask'], dtype=torch.float))\n",
        "  val_sampler = SequentialSampler(val_data)\n",
        "  val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "  return (tokenized_validation, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_validation, val_dataloader = get_val_dataloader(validation_squad, batch_size)"
      ],
      "metadata": {
        "id": "-4RkxmCrNh8k"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation requirements"
      ],
      "metadata": {
        "id": "PyhqSD4Vqhke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O dev-v2.0.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3E1qArBrPv0",
        "outputId": "33333fe4-2205-47bd-d03c-27ecf21264d1"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-15 23:43:15--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘dev-v2.0.json’\n",
            "\n",
            "dev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-15 23:43:15 (100 MB/s) - ‘dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(tokenized_validation, validation_squad, val_dataloader):\n",
        "  threshold = 1.0\n",
        "  epoch_i = 0\n",
        "  correct = 0\n",
        "  pred_dict = {}\n",
        "  na_prob_dict = {}\n",
        "\n",
        "  model.eval()\n",
        "  correct = 0\n",
        "  batch_val_losses = []\n",
        "  row = 0\n",
        "  for test_batch in tqdm(val_dataloader):\n",
        "      input_ids, segment_ids, masks = tuple(t.to(device) for t in test_batch)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          start_logits, end_logits = model(input_ids=input_ids,\n",
        "                                          token_type_ids=segment_ids,\n",
        "                                          attention_mask=masks,\n",
        "                                          return_dict=False)\n",
        "\n",
        "      start_logits = start_logits.detach().cpu()\n",
        "      end_logits = end_logits.detach().cpu()\n",
        "\n",
        "      for bidx in range(len(start_logits)):\n",
        "          start_scores = np.array(F.softmax(start_logits[bidx], dim = 0))\n",
        "          end_scores = np.array(F.softmax(end_logits[bidx], dim = 0))\n",
        "\n",
        "          size = len(start_scores)\n",
        "          scores = np.zeros((size, size))\n",
        "\n",
        "          for j in range(size):\n",
        "              for i in range(j+1):\n",
        "                  scores[i,j] = start_scores[i] + end_scores[j]\n",
        "\n",
        "          start_pred, end_pred = unravel_index(scores.argmax(), scores.shape)\n",
        "          answer_pred = \"\"\n",
        "          if (scores[start_pred, end_pred] > scores[0,0]+threshold):\n",
        "\n",
        "              offsets = tokenized_validation.offset_mapping[row]\n",
        "              pred_char_start = offsets[start_pred][0]\n",
        "\n",
        "              if end_pred < len(offsets):\n",
        "                  pred_char_end = offsets[end_pred][1]\n",
        "                  answer_pred = validation_squad[row]['context'][pred_char_start:pred_char_end]\n",
        "              else:\n",
        "                  answer_pred = validation_squad[row]['context'][pred_char_start:]\n",
        "\n",
        "              if answer_pred in validation_squad[row]['answers']['text']:\n",
        "                  correct += 1\n",
        "\n",
        "          else:\n",
        "              if (len(validation_squad[row]['answers']['text']) ==0):\n",
        "                  correct += 1\n",
        "\n",
        "          pred_dict[validation_squad[row]['id']] = answer_pred\n",
        "          na_prob_dict[validation_squad[row]['id']] = scores[0,0]\n",
        "\n",
        "          row+=1\n",
        "\n",
        "\n",
        "  accuracy = correct/validation_squad.num_rows\n",
        "  print(\"\\n\",\"accuracy is: \", accuracy)\n",
        "  with open(\"pred.json\", \"w\") as outfile:\n",
        "    json.dump(pred_dict, outfile)\n",
        "  with open(\"na_prob.json\", \"w\") as outfile:\n",
        "    json.dump(na_prob_dict, outfile)"
      ],
      "metadata": {
        "id": "SkGYzEMFrGrc"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "modified official evaluation script for SQuAD version 2\n",
        "\"\"\"\n",
        "import argparse\n",
        "import collections\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "ARTICLES_REGEX = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "\n",
        "OPTS = None\n",
        "\n",
        "def make_qid_to_has_ans(dataset):\n",
        "    qid_to_has_ans = {}\n",
        "    for article in dataset:\n",
        "        for p in article[\"paragraphs\"]:\n",
        "            for qa in p[\"qas\"]:\n",
        "                qid_to_has_ans[qa[\"id\"]] = bool(qa[\"answers\"][0][\"text\"] if qa[\"answers\"] else \"\")\n",
        "                #qid_to_has_ans[qa[\"id\"]] = bool(qa[\"answers\"][\"text\"])\n",
        "    return qid_to_has_ans\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return ARTICLES_REGEX.sub(\" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def get_raw_scores(dataset, preds):\n",
        "    exact_scores = {}\n",
        "    f1_scores = {}\n",
        "    for article in dataset:\n",
        "        for p in article[\"paragraphs\"]:\n",
        "            for qa in p[\"qas\"]:\n",
        "                qid = qa[\"id\"]\n",
        "                gold_answers = [normalize_answer(answer[\"text\"]) for answer in qa[\"answers\"] if answer.get(\"text\")]\n",
        "                #gold_answers = [t for t in qa[\"answers\"][\"text\"] if normalize_answer(t)]\n",
        "                if not gold_answers:\n",
        "                    # For unanswerable questions, only correct answer is empty string\n",
        "                    gold_answers = [\"\"]\n",
        "                if qid not in preds:\n",
        "                    #print(f\"Missing prediction for {qid}\")\n",
        "                    continue\n",
        "                a_pred = preds[qid]\n",
        "                # Take max over all gold answers\n",
        "                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
        "                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
        "    return exact_scores, f1_scores\n",
        "\n",
        "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
        "    new_scores = {}\n",
        "    for qid, s in scores.items():\n",
        "        pred_na = na_probs[qid] > na_prob_thresh\n",
        "        if pred_na:\n",
        "            new_scores[qid] = float(not qid_to_has_ans[qid])\n",
        "        else:\n",
        "            new_scores[qid] = s\n",
        "    return new_scores\n",
        "\n",
        "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
        "    if not qid_list:\n",
        "        total = len(exact_scores)\n",
        "        return collections.OrderedDict(\n",
        "            [\n",
        "                (\"exact\", 100.0 * sum(exact_scores.values()) / total),\n",
        "                (\"f1\", 100.0 * sum(f1_scores.values()) / total),\n",
        "                (\"total\", total),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        total = len(qid_list)\n",
        "        return collections.OrderedDict(\n",
        "            [\n",
        "                (\"exact\", 100.0 * sum(exact_scores.get(k, 0) for k in qid_list) / total),\n",
        "                (\"f1\", 100.0 * sum(f1_scores.get(k, 0) for k in qid_list) / total),\n",
        "                (\"total\", total),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "def merge_eval(main_eval, new_eval, prefix):\n",
        "    for k in new_eval:\n",
        "        main_eval[f\"{prefix}_{k}\"] = new_eval[k]\n",
        "\n",
        "def plot_pr_curve(precisions, recalls, out_image, title):\n",
        "    plt.step(recalls, precisions, color=\"b\", alpha=0.2, where=\"post\")\n",
        "    plt.fill_between(recalls, precisions, step=\"post\", alpha=0.2, color=\"b\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.xlim([0.0, 1.05])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.title(title)\n",
        "    plt.savefig(out_image)\n",
        "    plt.clf()\n",
        "\n",
        "def make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans, out_image=None, title=None):\n",
        "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "    true_pos = 0.0\n",
        "    cur_p = 1.0\n",
        "    cur_r = 0.0\n",
        "    precisions = [1.0]\n",
        "    recalls = [0.0]\n",
        "    avg_prec = 0.0\n",
        "    for i, qid in enumerate(qid_list):\n",
        "        if qid_to_has_ans[qid]:\n",
        "            true_pos += scores[qid]\n",
        "        cur_p = true_pos / float(i + 1)\n",
        "        cur_r = true_pos / float(num_true_pos)\n",
        "        if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i + 1]]:\n",
        "            # i.e., if we can put a threshold after this point\n",
        "            avg_prec += cur_p * (cur_r - recalls[-1])\n",
        "            precisions.append(cur_p)\n",
        "            recalls.append(cur_r)\n",
        "    if out_image:\n",
        "        plot_pr_curve(precisions, recalls, out_image, title)\n",
        "    return {\"ap\": 100.0 * avg_prec}\n",
        "\n",
        "def run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs, qid_to_has_ans, out_image_dir):\n",
        "    if out_image_dir and not os.path.exists(out_image_dir):\n",
        "        os.makedirs(out_image_dir)\n",
        "    num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n",
        "    if num_true_pos == 0:\n",
        "        return\n",
        "    pr_exact = make_precision_recall_eval(\n",
        "        exact_raw,\n",
        "        na_probs,\n",
        "        num_true_pos,\n",
        "        qid_to_has_ans,\n",
        "        out_image=os.path.join(out_image_dir, \"pr_exact.png\"),\n",
        "        title=\"Precision-Recall curve for Exact Match score\",\n",
        "    )\n",
        "    pr_f1 = make_precision_recall_eval(\n",
        "        f1_raw,\n",
        "        na_probs,\n",
        "        num_true_pos,\n",
        "        qid_to_has_ans,\n",
        "        out_image=os.path.join(out_image_dir, \"pr_f1.png\"),\n",
        "        title=\"Precision-Recall curve for F1 score\",\n",
        "    )\n",
        "    oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n",
        "    pr_oracle = make_precision_recall_eval(\n",
        "        oracle_scores,\n",
        "        na_probs,\n",
        "        num_true_pos,\n",
        "        qid_to_has_ans,\n",
        "        out_image=os.path.join(out_image_dir, \"pr_oracle.png\"),\n",
        "        title=\"Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)\",\n",
        "    )\n",
        "    merge_eval(main_eval, pr_exact, \"pr_exact\")\n",
        "    merge_eval(main_eval, pr_f1, \"pr_f1\")\n",
        "    merge_eval(main_eval, pr_oracle, \"pr_oracle\")\n",
        "\n",
        "def histogram_na_prob(na_probs, qid_list, image_dir, name):\n",
        "    if not qid_list:\n",
        "        return\n",
        "    x = [na_probs.get(k, 0.0) for k in qid_list]\n",
        "    #x = [na_probs[k] for k in qid_list]\n",
        "    weights = np.ones_like(x) / float(len(x))\n",
        "    plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n",
        "    plt.xlabel(\"Model probability of no-answer\")\n",
        "    plt.ylabel(\"Proportion of dataset\")\n",
        "    plt.title(f\"Histogram of no-answer probability: {name}\")\n",
        "    plt.savefig(os.path.join(image_dir, f\"na_prob_hist_{name}.png\"))\n",
        "    plt.clf()\n",
        "\n",
        "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
        "    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
        "    cur_score = num_no_ans\n",
        "    best_score = cur_score\n",
        "    best_thresh = 0.0\n",
        "    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "    for i, qid in enumerate(qid_list):\n",
        "        if qid not in scores:\n",
        "            continue\n",
        "        if qid_to_has_ans[qid]:\n",
        "            diff = scores[qid]\n",
        "        else:\n",
        "            if preds[qid]:\n",
        "                diff = -1\n",
        "            else:\n",
        "                diff = 0\n",
        "        cur_score += diff\n",
        "        if cur_score > best_score:\n",
        "            best_score = cur_score\n",
        "            best_thresh = na_probs[qid]\n",
        "    return 100.0 * best_score / len(scores), best_thresh\n",
        "\n",
        "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
        "    best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
        "    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
        "    main_eval[\"best_exact\"] = best_exact\n",
        "    main_eval[\"best_exact_thresh\"] = exact_thresh\n",
        "    main_eval[\"best_f1\"] = best_f1\n",
        "    main_eval[\"best_f1_thresh\"] = f1_thresh\n",
        "\n",
        "def evaluate():\n",
        "    with open(OPTS.data_file) as f:\n",
        "        dataset_json = json.load(f)\n",
        "        dataset = dataset_json[\"data\"]\n",
        "    with open(OPTS.pred_file) as f:\n",
        "        preds = json.load(f)\n",
        "    if OPTS.na_prob_file:\n",
        "        with open(OPTS.na_prob_file) as f:\n",
        "            na_probs = json.load(f)\n",
        "    else:\n",
        "        na_probs = {k: 0.0 for k in preds}\n",
        "    qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
        "    has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
        "    no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
        "    exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
        "    exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans, OPTS.na_prob_thresh)\n",
        "    f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans, OPTS.na_prob_thresh)\n",
        "    out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
        "    if has_ans_qids:\n",
        "        has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
        "        merge_eval(out_eval, has_ans_eval, \"HasAns\")\n",
        "    if no_ans_qids:\n",
        "        no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
        "        merge_eval(out_eval, no_ans_eval, \"NoAns\")\n",
        "    if OPTS.na_prob_file:\n",
        "        find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n",
        "    if OPTS.na_prob_file and OPTS.out_image_dir:\n",
        "        run_precision_recall_analysis(out_eval, exact_raw, f1_raw, na_probs, qid_to_has_ans, OPTS.out_image_dir)\n",
        "        histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, \"hasAns\")\n",
        "        histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, \"noAns\")\n",
        "    if OPTS.out_file:\n",
        "        with open(OPTS.out_file, \"w\") as f:\n",
        "            json.dump(out_eval, f)\n",
        "    else:\n",
        "        return json.dumps(out_eval, indent=2)\n",
        "\n",
        "OPTS = argparse.Namespace(\n",
        "  data_file=\"dev-v2.0.json\",  # Specify the path to your data file\n",
        "  pred_file=\"pred.json\",       # Specify the path to your predictions file\n",
        "  out_file=None,               # Set to None or specify the path for the output file\n",
        "  na_prob_file=\"na_prob.json\",  # Specify the path to your NA probability file\n",
        "  na_prob_thresh=1.0,\n",
        "  out_image_dir=\"./\",           # Specify the directory for saving images\n",
        "  verbose=False\n",
        ")\n",
        "\n",
        "if OPTS.out_image_dir:\n",
        "    import matplotlib\n",
        "    matplotlib.use(\"Agg\")\n",
        "    import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wLwCYsu0qhIf"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MReVWUdjhgla"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMsbG9k9wYkU",
        "outputId": "bc7c9322-48c8-4410-a926-b5a3e7cda839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "epochs = 10\n",
        "model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "UODH-qIDwYkU"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "      epoch_loss = []\n",
        "      validation_loss = []\n",
        "\n",
        "      total_loss = 0\n",
        "      model.train()\n",
        "\n",
        "      count=-1\n",
        "      progress_bar = tqdm(train_dataloader, leave=True, position=0)\n",
        "      progress_bar.set_description(f\"Epoch {epoch+1}\")\n",
        "      for batch in progress_bar:\n",
        "          count+=1\n",
        "          input_ids, segment_ids, mask, start, end  = tuple(t.to(device) for t in batch)\n",
        "\n",
        "          model.zero_grad()\n",
        "          loss, start_logits, end_logits = model(input_ids = input_ids,\n",
        "                                                  token_type_ids = segment_ids,\n",
        "                                                  attention_mask = mask,\n",
        "                                                  start_positions = start,\n",
        "                                                  end_positions = end,\n",
        "                                                  return_dict = False)\n",
        "\n",
        "          total_loss += loss.item()\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "          if (count % 20 == 0 and count != 0):\n",
        "              avg = total_loss/count\n",
        "              progress_bar.set_postfix(Loss=avg)\n",
        "\n",
        "      torch.save(model.state_dict(), \"./bert2_\" + str(epoch) + \".h5\")\n",
        "      avg_train_loss = total_loss / len(train_dataloader)\n",
        "      epoch_loss.append(avg_train_loss)\n",
        "      print(f\"Epoch {epoch} Loss: {avg_train_loss}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9C8vpgZN4VW",
        "outputId": "6af56179-a9e4-4938-a5c6-02b5f1f5bfd8"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 125/125 [01:29<00:00,  1.40it/s, Loss=4.46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 4.3984032783508304\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 125/125 [01:32<00:00,  1.34it/s, Loss=2.89]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 2.8474354438781737\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 125/125 [01:35<00:00,  1.30it/s, Loss=2.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 2.1738715052604674\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 125/125 [01:36<00:00,  1.29it/s, Loss=1.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 1.6258160290718078\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 125/125 [01:34<00:00,  1.32it/s, Loss=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 1.1767065677642823\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 125/125 [01:35<00:00,  1.31it/s, Loss=0.839]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 0.8262568665742874\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 125/125 [01:34<00:00,  1.32it/s, Loss=0.593]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 0.5842469004988671\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 125/125 [01:35<00:00,  1.31it/s, Loss=0.384]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 0.3772523908019066\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 125/125 [01:35<00:00,  1.31it/s, Loss=0.276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 0.27345848065614703\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 125/125 [01:34<00:00,  1.32it/s, Loss=0.228]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 0.22338134651631117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "do-qX2pEgCMU"
      },
      "source": [
        "## Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_accuracy(tokenized_validation, validation_squad, val_dataloader)"
      ],
      "metadata": {
        "id": "VNIhrUeO0oD9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628c6892-5c3d-4528-bfd6-200196861c2d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [01:34<00:00,  1.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.403\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate()"
      ],
      "metadata": {
        "id": "JHiz__ow0T-U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "26c15b13-441a-4356-aeb6-117917d3406e"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"exact\": 41.4,\\n  \"f1\": 43.29857142857144,\\n  \"total\": 1000,\\n  \"HasAns_exact\": 0.30364372469635625,\\n  \"HasAns_f1\": 0.6239155581260845,\\n  \"HasAns_total\": 5928,\\n  \"NoAns_exact\": 6.661059714045416,\\n  \"NoAns_f1\": 6.661059714045416,\\n  \"NoAns_total\": 5945,\\n  \"best_exact\": 594.8,\\n  \"best_exact_thresh\": 0.001625850098207593,\\n  \"best_f1\": 594.9316666666666,\\n  \"best_f1_thresh\": 0.0021326299756765366,\\n  \"pr_exact_ap\": 0.05518820667522107,\\n  \"pr_f1_ap\": 0.13709594709576495,\\n  \"pr_oracle_ap\": 4.6467076804282\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = json.loads(evaluate())\n",
        "print(f'F1 score = {evaluation[\"f1\"]}')"
      ],
      "metadata": {
        "id": "gwVE7rV-8P1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab847a74-687c-4555-8881-d786657c4491"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score = 43.29857142857144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Exact Match score = {evaluation[\"exact\"]}')"
      ],
      "metadata": {
        "id": "Fc9gqZxo80N2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f92d9c-64b7-4f5f-a2db-07afcd7b75a9"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match score = 41.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-d6RdZBV0qu-"
      },
      "source": [
        "## Curriculum Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWUZ44A832rf"
      },
      "source": [
        "#### Difficulty Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N = 10"
      ],
      "metadata": {
        "id": "bixrdKxDRmD_"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "j5cRRvtC4NPf"
      },
      "outputs": [],
      "source": [
        "subset_size = len(train_squad) // N\n",
        "shuffled_train_squad = train_squad.shuffle(seed=42)\n",
        "train_splits = [shuffled_train_squad.select(indices=range(i * subset_size, (i + 1) * subset_size)) for i in range(N)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subset_size = len(validation_squad) // N\n",
        "shuffled_validation_squad = validation_squad.shuffle(seed=42)\n",
        "validation_splits = [shuffled_validation_squad.select(indices=range(i * subset_size, (i + 1) * subset_size)) for i in range(N)]"
      ],
      "metadata": {
        "id": "X59K-hpncJP3"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "zoanfbM21qV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9541f05-1a44-4392-9e04-5905393f83e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Share 1, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.851925776554988\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.238157492417556\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.463050768925593\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 3.751704766200139\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.3096489356114316\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:08<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 2.9934418568244348\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.6615422322199893\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.323361974496108\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:07<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 1.9843842433049128\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.6055041276491606\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:05<00:00,  2.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.58\n",
            "Share 2, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.9117934520428\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.350228126232441\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.685936450958252\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 3.9848101689265323\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.479357884480403\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 3.04250876720135\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.652655784900372\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.2694855745022116\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 1.8073397874832153\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:08<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.3752739429473877\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:08<00:00,  1.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.49\n",
            "Share 3, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 6.090338963728684\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.494831231924204\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.932100259340727\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 4.319653731126052\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.8421063789954553\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 3.4133441631610575\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.986253628363976\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.55928020293896\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 2.157136531976553\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.727669330743643\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.44\n",
            "Share 4, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 6.056454658508301\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:09<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.289409197293795\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.565073270064134\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 3.962950798181387\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.601930783345149\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 3.177178841370803\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.8550300964942346\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.4049819157673764\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 2.0517704028349657\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.5729075532693129\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.54\n",
            "Share 5, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.662888343517597\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.125863735492413\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.56474205163809\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 4.03410423718966\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.3980347376603346\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 2.9706924511836124\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.5455264311570387\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.0545077874110294\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 1.5214072740994966\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.0611734252709608\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:09<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.5\n",
            "Share 6, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.975341430077186\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.3234316018911505\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.598797174600454\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 3.968320901577289\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.452544010602511\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 2.917267927756676\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.5034271570352407\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:09<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.1135324148031382\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 1.6134525354091938\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.1893563316418574\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:09<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.52\n",
            "Share 7, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.829115134019118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:06<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.228814345139724\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.590365923368013\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 3.9741879059718204\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.56036010155311\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 3.230900232608502\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.7936522227067213\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.486556053161621\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 2.032072351529048\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:06<00:00,  1.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.6245467754510732\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:09<00:00,  1.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.43\n",
            "Share 8, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.936285128960242\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.50904629780696\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 5.067943939795861\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 4.527000775704017\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:07<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 4.003667152844942\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 3.4480403936826267\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.9645719894996057\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:07<00:00,  1.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.550121307373047\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:07<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 1.9661319164129405\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.4823351456568792\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:07<00:00,  1.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.52\n",
            "Share 9, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.693298523242657\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.027247062096229\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.312583501522358\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 3.7060553293961744\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.354290943879348\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 2.938144188660842\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 2.6681164044600267\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.247289648422828\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 1.956817076756404\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.577721077662248\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:04<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.44\n",
            "Share 10, size = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Epoch 1: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 6.007621398338904\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 5.502606061788706\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 4.930734671079195\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 4.358292377912081\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 3.902817671115582\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 3.5056703640864444\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 3.162917595643264\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 2.7393771501687856\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 2.218337526688209\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 1.7829309243422289\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [00:09<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.61\n"
          ]
        }
      ],
      "source": [
        "difficulty_scores = []\n",
        "\n",
        "for i, (train_split, validation_split) in enumerate(zip(train_splits, validation_splits), 1):\n",
        "\n",
        "    print(f\"Share {i}, size = {len(train_split)}\")\n",
        "\n",
        "    tokenized_train_split, processed_train_split = process_train(train_split)\n",
        "    train_split_dataloader = get_train_dataloader(tokenized_train_split, processed_train_split, batch_size)\n",
        "\n",
        "    tokenized_split_validation, val_split_dataloader = get_val_dataloader(validation_split, batch_size)\n",
        "\n",
        "    #model\n",
        "    model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "    epochs = 10\n",
        "    model.to(device)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "    #calculate the difficulty score of each training example\n",
        "    train(train_split_dataloader)\n",
        "\n",
        "    #evaluate\n",
        "    calculate_accuracy(tokenized_split_validation, validation_split, val_split_dataloader)\n",
        "    eval = json.loads(evaluate())\n",
        "    difficulty_scores.append(eval[\"f1\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "difficulty_scores"
      ],
      "metadata": {
        "id": "4EnkMShueimO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0abdcc4-2d1a-4ab2-ff38-fd0b3e9d680c"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[58.0, 49.0, 44.0, 54.0, 50.0, 52.0, 43.0, 52.0, 44.0, 61.0]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Curriculum Arrangement"
      ],
      "metadata": {
        "id": "GhKmC2ml2krA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "model.to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwTYbKaoyo6k",
        "outputId": "2623fa46-0058-40c1-d16c-6cdb5ea7af69"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "GlXmz8p952u0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b462090-0c44-400a-dc9f-023b184cdd49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1, difficulty score = 43.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:06<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.66234592291025\n",
            "\n",
            "Stage 2, difficulty score = 44.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 5.533884525299072\n",
            "\n",
            "Stage 3, difficulty score = 44.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 4.825092975909893\n",
            "\n",
            "Stage 4, difficulty score = 49.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 4.287556391495925\n",
            "\n",
            "Stage 5, difficulty score = 50.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:07<00:00,  1.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 4.109731912612915\n",
            "\n",
            "Stage 6, difficulty score = 52.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 3.5619541314932017\n",
            "\n",
            "Stage 7, difficulty score = 52.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 3.9690938546107364\n",
            "\n",
            "Stage 8, difficulty score = 54.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:09<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 3.6029193584735575\n",
            "\n",
            "Stage 9, difficulty score = 58.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:07<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 3.2861941044147196\n",
            "\n",
            "Stage 10, difficulty score = 61.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 13/13 [00:09<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 3.652097463607788\n",
            "\n"
          ]
        }
      ],
      "source": [
        "stages = list(zip(train_splits, difficulty_scores))\n",
        "\n",
        "sorted_stages = sorted(stages, key=lambda x: x[1])\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "for i, (train_split, difficulty_score) in enumerate(sorted_stages, 1):\n",
        "  print(f\"Stage {i}, difficulty score = {difficulty_score}\")\n",
        "\n",
        "  tokenized_train_split, processed_train_split = process_train(train_split)\n",
        "  train_split_dataloader = get_train_dataloader(tokenized_train_split, processed_train_split, batch_size)\n",
        "\n",
        "  train(train_split_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "train(train_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhK26jnSpobW",
        "outputId": "1a8c1611-3cfa-47f1-ee4f-92aa2804477f"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 125/125 [01:34<00:00,  1.32it/s, Loss=2.83]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 2.7785153646469114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 125/125 [01:36<00:00,  1.30it/s, Loss=2.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 2.074807716846466\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 125/125 [01:35<00:00,  1.30it/s, Loss=1.55]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 1.5493745169639588\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 125/125 [01:36<00:00,  1.30it/s, Loss=1.07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 1.048546949148178\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 125/125 [01:34<00:00,  1.32it/s, Loss=0.736]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 0.7400834363698959\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 125/125 [01:34<00:00,  1.32it/s, Loss=0.503]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 0.4972257369160652\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 125/125 [01:35<00:00,  1.31it/s, Loss=0.348]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 0.34840413030982015\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 125/125 [01:35<00:00,  1.30it/s, Loss=0.222]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 0.2210002940967679\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 125/125 [01:35<00:00,  1.30it/s, Loss=0.154]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 0.16077140829712153\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 125/125 [01:35<00:00,  1.31it/s, Loss=0.139]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 0.1337820711405948\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_accuracy(tokenized_validation, validation_squad, val_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9EkV4pPqamw",
        "outputId": "069a037b-b665-4572-d970-6e82d6d7bccd"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 125/125 [01:35<00:00,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " accuracy is:  0.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "C3brH312qsdd",
        "outputId": "e7f49a34-2747-4833-ad65-459cb212667d"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\\n  \"exact\": 44.2,\\n  \"f1\": 46.12190476190477,\\n  \"total\": 1000,\\n  \"HasAns_exact\": 0.33738191632928477,\\n  \"HasAns_f1\": 0.661589872116188,\\n  \"HasAns_total\": 5928,\\n  \"NoAns_exact\": 7.098402018502943,\\n  \"NoAns_f1\": 7.098402018502943,\\n  \"NoAns_total\": 5945,\\n  \"best_exact\": 594.5,\\n  \"best_exact_thresh\": 0.0,\\n  \"best_f1\": 594.5,\\n  \"best_f1_thresh\": 0.0,\\n  \"pr_exact_ap\": 0.04978528437952521,\\n  \"pr_f1_ap\": 0.1562556621535656,\\n  \"pr_oracle_ap\": 4.698402914437673\\n}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = json.loads(evaluate())\n",
        "print(f'F1 score = {evaluation[\"f1\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBTMMXKVq6OP",
        "outputId": "8bbe497f-ee66-4f83-c9b4-19a036e27c9f"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score = 46.12190476190477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Exact Match score = {evaluation[\"exact\"]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdigdncsq60R",
        "outputId": "19ac2330-045e-40c6-a9ee-3412271b6b5a"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match score = 44.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results and Future Work"
      ],
      "metadata": {
        "id": "KPPnQZBEGkq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = {\n",
        "    \"exact_match\": 43.3,\n",
        "    \"f1\": 41.4,\n",
        "}\n",
        "\n",
        "cl_model = {\n",
        "    \"exact_match\": evaluation[\"exact\"],\n",
        "    \"f1\": evaluation[\"f1\"],\n",
        "}\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"Metric\": [\"EM\", \"F1\"],\n",
        "    \"No CL\": [model1[\"exact_match\"], model1[\"f1\"]],\n",
        "    \"CL\": [cl_model[\"exact_match\"], cl_model[\"f1\"]],\n",
        "})\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc97WhlwGkdu",
        "outputId": "0701a91d-cd46-44b0-8685-bde3cc00f547"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Metric  No CL         CL\n",
            "0     EM   43.3  44.200000\n",
            "1     F1   41.4  46.121905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  The result of this experiment is inline with the results described in the paper. Curriculum learning yielded a better performance on the machine reading comprehension task with higher F1 and exact match scores.\n",
        "*   Only a simple aspect of the results in the paper is reproduced in this experiment. So, with enough time and compute, future work can cover:\n",
        "  - Meticulously reproducing all the results from the paper spanning different NLU tasks, with the same exact details.\n",
        "  - Experimenting with variables and hyperparameters like the number of training stages N and the number of epochs to extend the results presented in the paper.\n",
        "  - Incorporate and merge different training paradigms into the curriculum learning framework.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DvD0Nj49Ta3e"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}